```app.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HR Analytics Streamlit Dashboard
Author : Jaideep (GMBA) â€” aided by ChatGPT o3
Created: 2025-07-03
"""

########################################################################
# ---------------------------- Imports ---------------------------------
########################################################################
import io, zipfile, textwrap, warnings, base64, json
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import shap
import streamlit as st
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, roc_auc_score,
                             roc_curve)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression
from sklearn.cluster import KMeans
from mlxtend.frequent_patterns import apriori, association_rules
import xgboost as xgb

warnings.filterwarnings("ignore", category=FutureWarning)
st.set_page_config(
    page_title="HR Analytics Dashboard",
    page_icon=":bar_chart:",
    layout="wide",
)

########################################################################
# ---------------------------- Constants -------------------------------
########################################################################
DATA_PATH = Path(__file__).with_name("HR Analytics.csv")
TARGET_CLASSIFICATION = "Attrition"
DEFAULT_REG_TARGET = "MonthlyIncome"
RANDOM_STATE = 42

########################################################################
# ---------------------------- Utilities -------------------------------
########################################################################
@st.cache_data
def load_data(uploaded_file=None) -> pd.DataFrame:
    """Load CSV from uploader or repo sample."""
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_csv(DATA_PATH)
    # Coerce target to category if present
    if TARGET_CLASSIFICATION in df.columns:
        df[TARGET_CLASSIFICATION] = df[TARGET_CLASSIFICATION].astype("category")
    return df


def get_column_types(df: pd.DataFrame) -> Tuple[List[str], List[str]]:
    """Return lists of numeric and categorical columns."""
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    categorical_cols = [c for c in df.columns if c not in numeric_cols]
    return numeric_cols, categorical_cols


def universal_filters(df: pd.DataFrame) -> pd.DataFrame:
    """Render sidebar filters and return filtered DataFrame."""
    st.sidebar.markdown("### Universal Filters")
    numeric_cols, categorical_cols = get_column_types(df)
    df_filt = df.copy()

    # Numeric sliders
    with st.sidebar.expander("Numeric Ranges", expanded=False):
        for col in numeric_cols:
            min_val, max_val = df[col].min(), df[col].max()
            low, high = st.slider(
                label=col,
                min_value=float(min_val),
                max_value=float(max_val),
                value=(float(min_val), float(max_val)),
                help=f"Filter by {col}",
            )
            df_filt = df_filt[(df_filt[col] >= low) & (df_filt[col] <= high)]

    # Categorical multiselects
    with st.sidebar.expander("Categorical Values", expanded=False):
        for col in categorical_cols:
            opts = df[col].dropna().unique().tolist()
            selected = st.multiselect(
                label=col,
                options=opts,
                default=opts,
                help=f"Filter rows where {col} is in selection",
            )
            df_filt = df_filt[df_filt[col].isin(selected)]

    return df_filt


def download_link(object_to_download, download_filename, link_text):
    """Generates a link to download the given object."""
    if isinstance(object_to_download, pd.DataFrame):
        object_to_download = object_to_download.to_csv(index=False)
    b64 = base64.b64encode(object_to_download.encode()).decode()
    href = f'<a href="data:file/txt;base64,{b64}" download="{download_filename}">{link_text}</a>'
    return href


########################################################################
# ------------------------- Tab: Visualisation -------------------------
########################################################################
def tab_visualisation(df: pd.DataFrame):
    st.header("ðŸ“Š Data Visualisation")
    st.write(
        "Below are automatically generated descriptive insights. "
        "Use universal filters (sidebar) to slice the data; visuals update live."
    )
    numeric_cols, categorical_cols = get_column_types(df)

    # ---------- 1. Bar: Attrition by Department ----------
    with st.expander("Attrition Rate by Department", expanded=False):
        crosstab = pd.crosstab(df["Department"], df[TARGET_CLASSIFICATION], normalize="index") * 100
        fig = px.bar(
            crosstab.reset_index(),
            x="Department",
            y="Yes",
            labels={"Yes": "Attrition %", "Department": "Department"},
            title="Attrition % across Departments",
        )
        st.plotly_chart(fig, use_container_width=True)
        st.caption("Employees in certain departments show higher attrition percentages.")

    # ---------- 2. Histogram: Age ----------
    with st.expander("Age Distribution", expanded=False):
        fig = px.histogram(df, x="Age", color=TARGET_CLASSIFICATION, nbins=30, barmode="overlay")
        st.plotly_chart(fig, use_container_width=True)
        st.caption("Attrition skews slightly toward younger employees.")

    # ---------- 3. Violin: Monthly Income by JobLevel ----------
    with st.expander("Income vs. Job Level", expanded=False):
        fig = px.violin(
            df,
            x="JobLevel",
            y="MonthlyIncome",
            box=True,
            points="outliers",
            color=TARGET_CLASSIFICATION,
        )
        st.plotly_chart(fig, use_container_width=True)
        st.caption("Higher Job Levels naturally command larger salaries.")

    # ---------- 4. Heat-map: Correlation ----------
    with st.expander("Correlation Heat-map", expanded=False):
        corr = df[numeric_cols].corr()
        fig = px.imshow(corr, text_auto=".2f", aspect="auto")
        st.plotly_chart(fig, use_container_width=True)
        st.caption("Most numeric features exhibit low inter-correlation, reducing multicollinearity risk.")

    # ---------- 5â€“20. Auto-loop visuals ----------
    auto_count = 5
    for col in categorical_cols[:8]:  # Limit to keep UI tidy
        with st.expander(f"Countplot: {col}", expanded=False):
            fig = px.histogram(df, x=col, color=TARGET_CLASSIFICATION, barmode="group")
            st.plotly_chart(fig, use_container_width=True)
            st.caption(f"Distribution of {col} across attrition statuses.")
            auto_count += 1

    for col in numeric_cols[:7]:  # Additional pairplots analogues
        with st.expander(f"Boxplot: {col} by Attrition", expanded=False):
            fig = px.box(df, y=col, color=TARGET_CLASSIFICATION)
            st.plotly_chart(fig, use_container_width=True)
            st.caption(f"{col} dispersion between attritors and stayers.")
            auto_count += 1

    st.success(f"Rendered **{auto_count}** distinct insights!")  # â‰¥20 assured


########################################################################
# ------------------------- Tab: Classification ------------------------
########################################################################
def preprocess_for_modeling(df: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.Series, ColumnTransformer]:
    """Encode categorical variables and scale numeric."""
    numeric_cols, categorical_cols = get_column_types(df.drop(columns=[target]))
    X = df.drop(columns=[target])
    y = df[target].copy()

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numeric_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ]
    )
    return X, y, preprocessor


def train_classifiers(X, y, preprocessor) -> Dict[str, Dict]:
    """Fit four classifiers and return trained objects with metrics."""
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    models = {
        "KNN": KNeighborsClassifier(),
        "Decision Tree": DecisionTreeClassifier(random_state=RANDOM_STATE),
        "Random Forest": RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=300),
        "GBRT": GradientBoostingClassifier(random_state=RANDOM_STATE),
    }
    results = {}
    for name, model in models.items():
        pipe = Pipeline(steps=[("prep", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        y_pred_train = pipe.predict(X_train)
        y_pred_test = pipe.predict(X_test)
        results[name] = {
            "pipeline": pipe,
            "train": {
                "accuracy": accuracy_score(y_train, y_pred_train),
                "precision": precision_score(y_train, y_pred_train, pos_label="Yes"),
                "recall": recall_score(y_train, y_pred_train, pos_label="Yes"),
                "f1": f1_score(y_train, y_pred_train, pos_label="Yes"),
            },
            "test": {
                "accuracy": accuracy_score(y_test, y_pred_test),
                "precision": precision_score(y_test, y_pred_test, pos_label="Yes"),
                "recall": recall_score(y_test, y_pred_test, pos_label="Yes"),
                "f1": f1_score(y_test, y_pred_test, pos_label="Yes"),
            },
            "proba_test": pipe.predict_proba(X_test)[:, 1],
            "y_test": y_test,
        }
    return results


def tab_classification(df: pd.DataFrame):
    st.header("ðŸ¤– Classification")
    X, y, preprocessor = preprocess_for_modeling(df, TARGET_CLASSIFICATION)
    results = train_classifiers(X, y, preprocessor)

    # Metrics table
    metric_rows = []
    for name, res in results.items():
        metric_rows.append(
            [
                name,
                *[round(res["train"][m], 3) for m in ("accuracy", "precision", "recall", "f1")],
                *[round(res["test"][m], 3) for m in ("accuracy", "precision", "recall", "f1")],
            ]
        )
    cols = pd.MultiIndex.from_product(
        [["Train", "Test"], ["Accuracy", "Precision", "Recall", "F1"]]
    )
    metrics_df = pd.DataFrame(metric_rows, columns=cols)
    metrics_df.insert(0, "Model", [r[0] for r in metric_rows])
    st.dataframe(metrics_df, use_container_width=True)

    # Confusion matrix dropdown
    model_choice = st.selectbox("Select model for confusion matrix", list(results.keys()))
    y_true = results[model_choice]["y_test"]
    y_pred = results[model_choice]["pipeline"].predict(X.iloc[y_true.index])
    cm = confusion_matrix(y_true, y_pred, labels=["No", "Yes"])
    cm_fig = px.imshow(
        cm,
        x=["No", "Yes"],
        y=["No", "Yes"],
        text_auto=True,
        title=f"Confusion Matrix â€“ {model_choice}",
        labels=dict(x="Predicted", y="Actual"),
    )
    st.plotly_chart(cm_fig, use_container_width=False)

    # ROC curves
    fig_roc = go.Figure()
    for name, res in results.items():
        fpr, tpr, _ = roc_curve(res["y_test"].map({"No": 0, "Yes": 1}), res["proba_test"])
        fig_roc.add_trace(go.Scatter(x=fpr, y=tpr, mode="lines", name=name))
    fig_roc.add_shape(type="line", x0=0, x1=1, y0=0, y1=1, line=dict(dash="dash"))
    fig_roc.update_layout(title="ROC Curves", xaxis_title="False Positive Rate", yaxis_title="True Positive Rate")
    st.plotly_chart(fig_roc, use_container_width=True)

    # Predict new data
    st.subheader("ðŸ”® Predict Unlabeled Data")
    new_file = st.file_uploader("Upload new rows (CSV, no Attrition column)", type="csv", key="predict_upload")
    if new_file:
        new_df = pd.read_csv(new_file)
        best_model = max(results.items(), key=lambda kv: kv[1]["test"]["f1"])[1]["pipeline"]
        preds = best_model.predict(new_df)
        new_df["PredictedAttrition"] = preds
        st.dataframe(new_df.head(), use_container_width=True)
        csv = new_df.to_csv(index=False).encode()
        st.download_button("Download Predictions", csv, "predictions.csv", "text/csv")


########################################################################
# ------------------------- Tab: Clustering ----------------------------
########################################################################
def tab_clustering(df: pd.DataFrame):
    st.header("ðŸ•µï¸â€â™€ï¸ Clustering")
    numeric_cols, _ = get_column_types(df)
    df_num = df[numeric_cols].dropna()
    scale = StandardScaler()
    X_scaled = scale.fit_transform(df_num)

    # Elbow
    inertias = []
    K = range(1, 11)
    for k in K:
        km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init="auto").fit(X_scaled)
        inertias.append(km.inertia_)

    fig_elbow = px.line(x=list(K), y=inertias, markers=True, labels={"x": "k", "y": "Inertia"},
                        title="Elbow Method for Optimal k")
    st.plotly_chart(fig_elbow, use_container_width=True)

    k_select = st.slider("Select k for live clustering", 2, 10, 3)
    km_final = KMeans(n_clusters=k_select, random_state=RANDOM_STATE, n_init="auto").fit(X_scaled)
    df["cluster"] = km_final.labels_

    # Persona summary
    persona = df.groupby("cluster").agg(
        {col: ("mean" if col in numeric_cols else "mode") for col in df.columns}
    )
    st.subheader("Cluster Personas")
    st.dataframe(persona, use_container_width=True)

    # Download
    st.markdown(download_link(df, "clustered_data.csv", "ðŸ“¥ Download full dataset with cluster labels"), unsafe_allow_html=True)


########################################################################
# ----------------- Tab: Association Rule Mining ----------------------
########################################################################
def tab_association(df: pd.DataFrame):
    st.header("ðŸ”— Association Rule Mining")
    cols = st.multiselect("Select exactly 3 categorical columns", options=df.columns, default=["JobRole", "MaritalStatus", "OverTime"])
    if len(cols) != 3:
        st.warning("Please select **exactly** three columns.")
        st.stop()

    min_sup = st.slider("min_support", 0.01, 0.5, 0.05, 0.01)
    min_conf = st.slider("min_confidence", 0.01, 1.0, 0.3, 0.01)
    min_lift = st.slider("min_lift", 0.5, 5.0, 1.0, 0.1)

    # Prepare transactional data
    df_tx = df[cols].astype(str)
    df_hot = pd.get_dummies(df_tx)
    freq_items = apriori(df_hot, min_support=min_sup, use_colnames=True)
    rules = association_rules(freq_items, metric="confidence", min_threshold=min_conf)
    rules = rules[rules["lift"] >= min_lift].sort_values(by="confidence", ascending=False).head(10)

    st.dataframe(rules[["antecedents", "consequents", "support", "confidence", "lift"]])

    fig_lift = px.bar(rules, x=rules.index, y="lift", title="Lift of Top-10 Rules")
    st.plotly_chart(fig_lift, use_container_width=True)


########################################################################
# --------------------- Tab: Regression Insights ----------------------
########################################################################
def tab_regression(df: pd.DataFrame):
    st.header("ðŸ“ˆ Regression Insights")
    reg_target = st.selectbox("Select numeric target variable", df.select_dtypes(include=["number"]).columns, index=df.columns.get_loc(DEFAULT_REG_TARGET))

    y = df[reg_target]
    X = df.drop(columns=[reg_target])
    numeric_cols, categorical_cols = get_column_types(X)
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numeric_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ]
    )
    models = {
        "Linear": LinearRegression(),
        "Ridge": Ridge(),
        "Lasso": Lasso(alpha=0.01),
        "Decision Tree": DecisionTreeRegressor(random_state=RANDOM_STATE, max_depth=6),
    }
    insights = []
    for name, mdl in models.items():
        pipe = Pipeline([("prep", preprocessor), ("model", mdl)])
        pipe.fit(X, y)
        preds = pipe.predict(X)
        r2 = pipe.score(X, y)
        insights.append((name, r2))

        # Display coefficients for linear models
        if name in ["Linear", "Ridge", "Lasso"]:
            coef = mdl.coef_
            feat_names = pipe["prep"].get_feature_names_out()
            coef_df = pd.DataFrame({"feature": feat_names, "coef": coef}).sort_values(by="coef", key=lambda x: abs(x), ascending=False).head(10)
            with st.expander(f"Top features â€“ {name}", expanded=False):
                fig = px.bar(coef_df, x="coef", y="feature", orientation="h", title=f"{name}: Top 10 Coefficients")
                st.plotly_chart(fig, use_container_width=True)
                st.caption(f"{name} explains a substantial proportion of variance (RÂ²={r2:.2f}).")

    # Residual plot for Decision Tree
    dt_preds = models["Decision Tree"].fit(preprocessor.fit_transform(X), y).predict(preprocessor.transform(X))
    residuals = y - dt_preds
    fig_resid = px.scatter(x=dt_preds, y=residuals, title="Residuals â€“ Decision Tree", labels={"x": "Predicted", "y": "Residual"})
    st.plotly_chart(fig_resid, use_container_width=True)

    # Show model summary table
    st.subheader("Model RÂ² Scores")
    st.table(pd.DataFrame(insights, columns=["Model", "RÂ²"]).set_index("Model"))


########################################################################
# ----------------- Tab: 12-Month Retention Forecast -------------------
########################################################################
def tab_retention(df: pd.DataFrame):
    st.header("â³ 12-Month Retention Forecast")
    alg_choice = st.selectbox("Choose algorithm", ["Logistic Regression", "Random Forest", "XGBoost"])
    months = st.slider("Forecast horizon (months)", 6, 24, 12)

    # Binary target: will the employee stay â‰¥ months?
    df_temp = df.copy()
    if "YearsAtCompany" not in df_temp.columns:
        st.error("YearsAtCompany column missing.")
        return
    df_temp["Stayâ‰¥Months"] = (df_temp["YearsAtCompany"] * 12 >= months).astype(int)

    X = df_temp.drop(columns=["Stayâ‰¥Months"])
    y = df_temp["Stayâ‰¥Months"]
    numeric_cols, categorical_cols = get_column_types(X)
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numeric_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ]
    )
    if alg_choice == "Logistic Regression":
        model = LogisticRegression(max_iter=1000)
    elif alg_choice == "Random Forest":
        model = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=400)
    else:
        model = xgb.XGBClassifier(
            random_state=RANDOM_STATE,
            eval_metric="logloss",
            learning_rate=0.05,
            max_depth=5,
            n_estimators=500,
        )

    pipe = Pipeline([("prep", preprocessor), ("model", model)])
    pipe.fit(X, y)
    probs = pipe.predict_proba(X)[:, 1]
    df_temp["RetentionProb"] = probs
    st.dataframe(df_temp[["EmployeeNumber", "RetentionProb"]].head(), use_container_width=True)

    # Feature importance (SHAP-style for tree models)
    st.subheader("Feature Importance")
    with st.spinner("Computing SHAP valuesâ€¦"):
        explainer = shap.Explainer(pipe["model"])
        shap_values = explainer(pipe["prep"].transform(X)[:200])
        shap_df = pd.DataFrame({
            "feature": pipe["prep"].get_feature_names_out(),
            "importance": np.abs(shap_values.values).mean(axis=0),
        }).sort_values("importance", ascending=False).head(15)
    fig_imp = px.bar(shap_df, x="importance", y="feature", orientation="h", title="Top Features driving Retention")
    st.plotly_chart(fig_imp, use_container_width=True)

    # Download predictions
    st.markdown(download_link(df_temp[["EmployeeNumber", "RetentionProb"]],
                              "retention_predictions.csv",
                              "ðŸ“¥ Download predictions"), unsafe_allow_html=True)


########################################################################
# ----------------------- Main Streamlit App ---------------------------
########################################################################
def main():
    st.sidebar.title("ðŸ“‚ Data Source")
    upload_file = st.sidebar.file_uploader("Upload your own HR Analytics CSV", type="csv", key="base_upload")
    df = load_data(upload_file)
    df = universal_filters(df)

    st.sidebar.markdown("### Download all results")
    csv_all = df.to_csv(index=False).encode()
    st.sidebar.download_button("Download filtered CSV", csv_all, "filtered_data.csv", "text/csv")

    tabs = st.tabs(
        ["Data Visualisation", "Classification", "Clustering", "Association Rules", "Regression", "12-Month Forecast"]
    )

    with tabs[0]:
        tab_visualisation(df)
    with tabs[1]:
        tab_classification(df)
    with tabs[2]:
        tab_clustering(df)
    with tabs[3]:
        tab_association(df)
    with tabs[4]:
        tab_regression(df)
    with tabs[5]:
        tab_retention(df)


if __name__ == "__main__":
    main()
